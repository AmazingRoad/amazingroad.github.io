<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haiyang Sun</title>
  
  <meta name="author" content="Haiyang Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Haiyang Sun | Â≠ôÊµ∑Ê¥ã</name>
                </p>
                <p style="font-size: 16px;font-style:italic; line-height:1.5;">
                    I am an Algorithm Expert at <strong><a href="https://www.xiaomiev.com/" style="font-size: 16px;">Xiaomi EV</a></strong> , primarily focused on <span class="highlight">Driving World Models</span> and their industrial applications. 
                <br>
                <br>
                    I am currently focused on the practical application of world models for <span class="highlight"><strong>SDG</strong> (synthetic data generation)</span> in industrial scenarios. 
                    Previously, I conducted research on world models and perception models for autonomous driving at <strong><a href="https://www.lixiang.com/en" style="font-size: 16px;">LiAuto</a></strong>.
                    Before that, I worked on data closed-loop systems at <strong><a href="https://damo.alibaba.com/?language=zh" style="font-size: 16px;">Alibaba DAMO Academy</a></strong>.
                <br>
                <br>
                    I'm technically interested in computer vision, particularly 3D scene reconstruction and generation, such as <span class="highlight">NeRF, 3DGS and Diffusion Models</span>.
                </p>
                </p>
                <p style="text-align:center;font-size: 16px;">
                  <a href="mailto:AmazingRoad@163.com">Email</a> &nbsp/&nbsp
                  <a href="https://github.com/wm-research">Github</a> &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?user=SYbFNsIAAAAJ&hl=zh-CN">Google Scholar</a>
                </p>
              </td>
              <td style="padding:1.0%;width:40%;max-width:40%">
                <a href="images/shy.jpeg"><img style="width:70%;max-width:80%" alt="profile photo" src="images/shy.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <p style="font-size: 16px;font-style:italic; line-height:1.2;">
                    [üéâ2025/09/18] Two papers accepted by NeurIPS 2025 (Genesis, Pixel-perfect Depth)
                    <br>
                    [üèÜ2025/09/15] Winner of <a href="https://huggingface.co/spaces/XDimLab/ICCV2025-RealADSim-NVS" style="font-size: 16px;"> the RealADSim workshop </a> @ ICCV 2025
                     <br>
                    [üéâ2025/06/26] One paper accepted by ICCV 2025 (3DRealCar)
                    <br>
                    [üéâ2025/06/16] One paper accepted by IROS 2025 as oral presentation (PosePilot)
                     <br>
                    [üéâ2025/05/01] One paper accepted by ICML 2025 (S2-Track)
                    <br>
                    [üéâ2024/12/20] One paper accepted by RA-L (BEV-TSR)
                    <br>
                    [üéâ2024/12/10] One paper accepted by AAAI 2025 (BEV-TSR)
                    <br>
                    [üèÜ2025/08/16] Winner of <a href="https://coda-dataset.github.io/w-coda2024/track2/" style="font-size: 16px;"> the W-CODA workshop </a> @ ECCV 2024
                    <br>
                    [üéâ2024/07/04] Three papers accepted by ECCV 2024 (StreetGaussians, OpenSight, TOD3Cap)
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Publications</heading>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/genesis.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency</papertitle>
                <br>
                Xiangyu Guo, Zhanqian Wu, Kaixin Xiong, Ziyang Xu, Lijun Zhou, Gangwei Xu, Shaoqing Xu, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Hangjun Ye, Wenyu Liu, Xinggang Wang
                <br>
                <em> Conference on Neural Information Processing Systems (NeurIPS)</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2506.07497">Paper</a> / 
                <a href="https://github.com/xiaomi-research/genesis">Code</a>
                <p></p>
              </td>
           </tr>
           <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/3drealcar.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>3drealcar: An in-the-wild rgb-d car dataset with 360-degree views</papertitle>
                <br>
                Xiaobiao Du, Yida Wang, <strong>Haiyang Sun</strong>, Zhuojie Wu, Hongwei Sheng, Shuyun Wang, Jiaying Ying, Ming Lu, Tianqing Zhu, Kun Zhan, Xin Yu 
                <br>
                <em> International Conference on Computer Vision (ICCV)</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2406.04875">Paper</a> / 
                <a href="https://github.com/xiaobiaodu/3DRealCar_Toolkit">Code</a>
                <p></p>
              </td>
           </tr>
           <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/posepolit.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth</papertitle>
                <br>
                Bu Jin, Weize Li, Baihan Yang, Zhenxin Zhu, Junpeng Jiang, Huan-ang Gao, <strong>Haiyang Sun</strong>, Kun Zhan, Hengtong Hu, Xueyang Zhang, Peng Jia, Hao Zhao
                <br>
                <em> International Conference on Intelligent Robots and Systems (IROS)</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2505.01729">Paper</a>
                <p></p>
              </td>
           </tr>
            <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/s2track.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object Tracking</papertitle>
                <br>
                Tao Tang, Lijun Zhou, Pengkun Hao, Zihang He, Kalok Ho, Shuo Gu, Zhihui Hao, <strong>Haiyang Sun</strong>, Kun Zhan, Peng Jia, XianPeng Lang, Xiaodan Liang 
                <br>
                <em> International Conference on Machine Learning (ICML)</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2406.02147">Paper</a>
                <p></p>
              </td>
           </tr>
            <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/BEV-TSR.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Bev-tsr: Text-scene retrieval in bev space for autonomous driving</papertitle>
                <br>
                Tao Tang, Dafeng Wei, Zhengyu Jia, Tian Gao, Changwei Cai, Chengkai Hou, Peng Jia, Kun Zhan, <strong>Haiyang Sun</strong>, Fan JingChen, Yixing Zhao, Xiaodan Liang, Xianpeng Lang, Yang Wang               
                <br>
                <em> Conference on Artificial Intelligence (AAAI)</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2401.01065">Paper</a>
                <p></p>
              </td>
           </tr>
           <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/dreamcar.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>DreamCar: Leveraging Car-Specific Prior for In-the-Wild 3D Car Reconstruction</papertitle>
                <br>
                Xiaobiao Du, <strong>Haiyang Sun</strong>, Ming Lu, Tianqing Zhu, Xin Yu
                <br>
                <em> IEEE Robotics and Automation Letters (RA-L)</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2407.16988">Paper</a>
                <p></p>
              </td>
           </tr>
            <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/StreetGS.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Street Gaussians: Modeling Dynamic Urban Scenes with Gaussian Splatting</papertitle>
                <br>
                Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, <strong>Haiyang Sun</strong>, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, Sida Peng.
                <br>
                <em> European Conference on Computer Vision (ECCV)</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2401.01339">Paper</a> /
                <a href="https://github.com/zju3dv/street_gaussians">code</a> 
                <p></p>
              </td>
           </tr>
           <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/TOD3Cap.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes</papertitle>
                <br>
                Bu Jin, Yupeng Zheng, Pengfei Li, Weize Li, Yuhang Zheng, Sujie Hu, Xinyu Liu, Jinwei Zhu, Zhijie Yan,<strong>Haiyang Sun</strong>, Kun Zhan, Peng Jia, Xiaoxiao Long, Yilun Chen, Hao Zhao
                <br>
                <em> European Conference on Computer Vision (ECCV)</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2403.19589">Paper</a> /
                <a href="https://github.com/jxbbb/TOD3Cap">code</a> 
                <p></p>
              </td>
           </tr>
           <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/Open-sight.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>OpenSight: A simple open-vocabulary framework for LiDAR-based object detection</papertitle>
                <br>
                Hu Zhang, Jianhua Xu, Tao Tang, <strong>Haiyang Sun</strong>, Xin Yu, Zi Huang, Kaicheng Yu                
                <em> European Conference on Computer Vision (ECCV)</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2312.08876">Paper</a>
                <p></p>
              </td>
           </tr>
          </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Competition</heading>
                <p>
                <!--My research interests are in computer vision&#x1F441; and robotics&#x1f916. Recently I am studying deep learning methods on object pose estimation for point cloud and RGB data. -->
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/Dive.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Dive: Dit-based video generation with enhanced control</papertitle>
                <br>
                Junpeng Jiang, Gangyi Hong, Lijun Zhou, Enhui Ma, Hengtong Hu, Xia Zhou, Jie Xiang, Fan Liu, Kaicheng Yu, <strong>Haiyang Sun</strong>, Kun Zhan, Peng Jia, Miao Zhang
                <br>
                <em>[üèÜ<strong>champion</strong>!] Winner of <a href="https://coda-dataset.github.io/w-coda2024/track2/"> the W-CODA workshop </a> @ ECCV 2024 </em>, 
                <br>
                <a href="https://arxiv.org/pdf/2409.01595">Paper</a>
                <p></p>
              </td>
           </tr>

          </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Others</heading>
            <p>
                papers under review
            </p>
            </td>
            </tr>
          </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/worldsplat.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for Autonomous Driving</papertitle>
                <br>
                Ziyue Zhu, Zhanqian Wu, Zhenxin Zhu, Lijun Zhou, <strong>Haiyang Sun</strong>‚Ä†, Bing Wan, Kun Ma, Guang Chen, Hangjun Ye, Jin Xie
                <br>
                <em> arXiv</em>, 2025
                <br>
                <a href="https://www.arxiv.org/abs/2509.23402">Paper</a> /
                <a href="https://github.com/wm-research/worldsplat">code</a> 
                <p></p>
              </td>
           </tr>
           <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/extrags.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>ExtraGS: Geometric-Aware Trajectory Extrapolation with Uncertainty-Guided Generative Priors</papertitle>
                <br>
                Kaiyuan Tan, Yingying Shen, Haohui Zhu, Zhiwei Zhan, Shan Zhao, Mingfei Tu, Hongcheng Luo, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Hangjun Ye
                <br>
                <em> arXiv</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2508.15529">Paper</a> /
                <a href="https://github.com/wm-research/extrags">code</a> 
                <p></p>
              </td>
           </tr>
            <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/recogdrive.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving</papertitle>
                <br>
                Yongkang Li, Kaixin Xiong, Xiangyu Guo, Fang Li, Sixu Yan, Gangwei Xu, Lijun Zhou, Long Chen, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Hangjun Ye, Wenyu Liu, Xinggang Wang
                <br>
                <em> arXiv</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2506.08052">Paper</a> /
                <a href="https://github.com/xiaomi-research/recogdrive">code</a> 
                <p></p>
              </td>
           </tr>
           <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/drivemrp.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction</papertitle>
                <br>
                Zhiyi Hou, Enhui Ma, Fang Li, Zhiyi Lai, Kalok Ho, Zhanqian Wu, Lijun Zhou, Long Chen, Chitian Sun, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Hangjun Ye, Kaicheng Yu
                <br>
                <em> arXiv</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2507.02948">Paper</a>
                <p></p>
              </td>
           </tr>
           <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/uni-gaussians.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Uni-gaussians: Unifying camera and lidar simulation with gaussians for dynamic driving scenarios</papertitle>
                <br>
                Zikang Yuan, Yuechuan Pu, Hongcheng Luo, Fengtian Lang, Cheng Chi, Teng Li, Yingying Shen, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Xin Yang
                <br>
                <em> arXiv</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2503.08317">Paper</a> / 
                <a href="https://xiaomi-research.github.io/uni-gaussians/">code</a> 
                <p></p>
              </td>
           </tr>
           <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/bat.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>BAT: Learning Event-based Optical Flow with Bidirectional Adaptive Temporal Correlation</papertitle>
                <br>
                Gangwei Xu, Haotong Lin, Zhaoxing Zhang, Hongcheng Luo, <strong>Haiyang Sun</strong>, Xin Yang
                <br>
                <em> arXiv</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2503.03256">Paper</a>
                <p></p>
              </td>
           </tr>
           <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/cogen.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Cogen: 3d consistent video generation via adaptive conditioning for autonomous driving</papertitle>
                <br>
                Yishen Ji, Ziyue Zhu, Zhenxin Zhu, Kaixin Xiong, Ming Lu, Zhiqi Li, Lijun Zhou, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Tong Lu
                <br>
                <em> arXiv</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2503.22231">Paper</a>
                <p></p>
              </td>
           </tr>
            <tr>
              <td style="padding:10px;width:30%;vertical-align:middle">
                <img src='images/Delphi.png' width="100%">
              </td>
              <td style="padding:20px;width:70%;vertical-align:middle">
                <papertitle>Unleashing generalization of end-to-end autonomous driving with controllable long video generation</papertitle>
                <br>
                Enhui Ma, Lijun Zhou, Tao Tang, Zhan Zhang, Dong Han, Junpeng Jiang, Kun Zhan, Peng Jia, Xianpeng Lang, <strong>Haiyang Sun</strong>, Di Lin, Kaicheng Yu
                <br>
                <em> arXiv</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2406.01349">Paper</a> /
                <a href="https://github.com/westlake-autolab/Delphi">code</a> 
                <p></p>
              </td>
           </tr>
          </tbody></table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Experience</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="10"><tbody>
            <tr>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/xiaomiEV-logo.png", width="90%"></td>
              <td width="80%" valign="center">
                <b>Xiaomi EV | Â∞èÁ±≥Ê±ΩËΩ¶</b>, China
                <br> 2024.09 - now
                <br>
                <br> <b>Lead Algorithm Expert</b>
              </td>
            </tr>
            <tr>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/LiAuto-logo.png", width="90%"></td>
              <td width="80%" valign="center">
                <b>LiAuto | ÁêÜÊÉ≥Ê±ΩËΩ¶</b>, China
                <br> 2022.12 - 2024.09
                <br>
                <br> <b>Senior Algorithm Expert</b>
              </td>
            </tr>
            <tr>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/alibaba-damo-logo.webp", width="90%"></td>
              <td width="80%" valign="center">
                <b>Alibaba DAMO Academy | ÈòøÈáåËææÊë©Èô¢</b>, China
                <br> 2017.12 - 2022.12
                <br>
                <br> <b>Algorithm Expert</b>
              </td>
            </tr>
            <tr>
            <tr>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/enhang-logo.jpg", width="90%"></td>
              <td width="80%" valign="center">
                <b>EHang | ‰∫øËà™Êô∫ËÉΩ</b>, China
                <br> 2016.07 - 2017.12
                <br>
                <br> <b>Senior Algorithm Engineer</b>
              </td>
            </tr>
            <tr>
            <tr>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/tsinghua_logo.png", width="90%"></td>
              <td width="80%" valign="center">
                <b>Tsinghua University</b>, China
                <br> 2013.09 - 2016.07
                <br>
                <br> <b>M.S. Student in Electronic Engineering</b>
              </td>
            </tr>
            <tr>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/BUPT-logo.png", width="90%"></td>
              <td width="80%" valign="center">
                <b>Beijing University of Posts and Telecommunications</b>, China
                <br> 2009.09 - 2013.07
                <br>
                <br> <b>B.S. in Information and Communication Engineering</b>
              </td>
            </tr>
            
  
          </tbody></table>
  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                Template stolen from <a href="https://jonbarron.info/">Jon Barron</a>.
                <br> Last updated: 03/05/2025
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
  
  </html>
  
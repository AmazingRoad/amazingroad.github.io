<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haiyang Sun</title>

  <meta name="author" content="Haiyang Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Haiyang Sun | Â≠ôÊµ∑Ê¥ã</name>
                  </p>
                  <p style="font-size: 16px;font-style:italic;">


                    I am an <b>Algorithm Specialist at Xiaomi EV</b>, working on <b>world models for autonomous
                      driving</b>. I focus on turning cutting-edge research into <b>production systems</b>, enabling
                    applications such as <b>closed-loop simulation</b>, <b>synthetic data generation</b>, and
                    <b>closed-loop training</b>.
                    <br>
                    <br>


                    Previously, I was the <b>World Model Tech Lead at Li Auto</b>, where I defined the technical roadmap
                    and built the team from the ground up, leading to the <b>large-scale deployment of production
                      closed-loop simulation systems</b>.
                    <br>
                    <br>


                    Earlier, at <b>Alibaba DAMO Academy</b>, I worked on <b>cloud-based data closed-loop algorithms</b>,
                    which shaped my long-term interest in <b>scalable learning systems</b>.
                    <br>
                    <br>


                    I have published <b>10+ papers</b> at top conferences including <b>CVPR, ICCV, and NeurIPS</b>, and
                    won multiple <b>international challenges</b>. I am passionate about building systems that truly work
                    in the real world.


                  </p>
                  <p style="text-align:center;font-size: 16px;">
                    <a href="mailto:AmazingRoad@163.com">Email</a> &nbsp/&nbsp
                    <a href="https://github.com/wm-research">Github</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=SYbFNsIAAAAJ&hl=zh-CN">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://www.zhihu.com/people/amazingroad">Áü•‰πé</a> &nbsp/&nbsp
                    <a href="https://www.xiaohongshu.com/user/profile/5ddd47a3000000000100ac3b">Â∞èÁ∫¢‰π¶</a>
                  </p>
                </td>
                <td style="padding:1.0%;width:40%;max-width:40%">
                  <a href="images/shy.jpeg"><img style="width:70%;max-width:80%" alt="profile photo"
                      src="images/shy.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                </tbody>
              </table>

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <heading>News</heading>
                      <p style="font-size: 16px;font-style:italic; line-height:1.2;">
                        [üéâ2026/01/26] Three papers accepted by ICLR 2026 (Dream4Drive, ReCogDrive, WorldSplat)
                        <br>
                        [üéâ2025/11/25] One paper accepted by TPAMI (Street Gaussians)
                        <br>
                        [üéâ2025/11/08] Two papers accepted by AAAI 2026 (BAT, CorrectAD/Delphi)
                        <br>
                        [üéâ2025/09/18] Two papers accepted by NeurIPS 2025 (Genesis, Pixel-perfect Depth)
                        <br>
                        [üèÜ2025/09/15] Winner of <a href="https://huggingface.co/spaces/XDimLab/ICCV2025-RealADSim-NVS"
                          style="font-size: 16px;"> the RealADSim workshop </a> @ ICCV 2025
                        <br>
                        [üéâ2025/06/26] One paper accepted by ICCV 2025 (3DRealCar)
                        <br>
                        [üéâ2025/06/16] One paper accepted by IROS 2025 as oral presentation (PosePilot)
                        <br>
                        [üéâ2025/05/01] One paper accepted by ICML 2025 (S2-Track)
                        <br>
                        [üéâ2024/12/20] One paper accepted by RA-L (DreamCar)
                        <br>
                        [üéâ2024/12/10] One paper accepted by AAAI 2025 (BEV-TSR)
                        <br>
                        [üèÜ2024/08/16] Winner of <a href="https://coda-dataset.github.io/w-coda2024/track2/"
                          style="font-size: 16px;"> the W-CODA workshop </a> @ ECCV 2024
                        <br>
                        [üéâ2024/07/04] Three papers accepted by ECCV 2024 (StreetGaussians, OpenSight, TOD3Cap)
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <heading>Publications</heading>
                    </td>
                  </tr>
                </tbody>
              </table>
              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/Dream4Drive.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks
                      </papertitle>
                      <br>
                      Kai Zeng, Zhanqian Wu, Kaixin Xiong, Xiaobao Wei, Xiangyu Guo, Zhenxin Zhu, Kalok Ho, Lijun Zhou,
                      Bohan Zeng, Ming Lu, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Hangjun Ye, Wentao
                      Zhang
                      <br>
                      <em> International Conference on Learning Representations (ICLR)</em>, 2026
                      <br>
                      <a href="https://arxiv.org/abs/2510.19195">Paper</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/worldsplat.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for Autonomous Driving
                      </papertitle>
                      <br>
                      Ziyue Zhu, Zhanqian Wu, Zhenxin Zhu, Lijun Zhou, <strong>Haiyang Sun</strong>‚Ä†, Bing Wan, Kun Ma,
                      Guang Chen, Hangjun Ye, Jin Xie
                      <br>
                      <em> International Conference on Learning Representations (ICLR)</em>, 2026
                      <br>
                      <a href="https://www.arxiv.org/abs/2509.23402">Paper</a> /
                      <a href="https://github.com/wm-research/worldsplat">code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/recogdrive.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving
                      </papertitle>
                      <br>
                      Yongkang Li, Kaixin Xiong, Xiangyu Guo, Fang Li, Sixu Yan, Gangwei Xu, Lijun Zhou, Long Chen,
                      <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Hangjun Ye, Wenyu Liu, Xinggang Wang
                      <br>
                      <em> International Conference on Learning Representations (ICLR)</em>, 2026
                      <br>
                      <a href="https://arxiv.org/abs/2506.08052">Paper</a> /
                      <a href="https://github.com/xiaomi-research/recogdrive">code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/StreetGS.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Street Gaussians: Modeling Dynamic Urban Scenes With Gaussian Primitives</papertitle>
                      <br>
                      Sida Peng, Yushi Long, Yunzhi Yan, Haotong Lin, Chenxu Zhou, <strong>Haiyang Sun</strong>, Kun
                      Zhan, Xianpeng Lang, Hujun Bao, Xiaowei Zhou
                      <br>
                      <em> IEEE transactions on pattern analysis and machine intelligence</em>
                      <br>
                      <a href="https://pubmed.ncbi.nlm.nih.gov/41259160/">Paper</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/CorrectAD.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in
                        Autonomous Driving</papertitle>
                      <br>
                      Enhui Ma, Lijun Zhou, Tao Tang, Jiahuan Zhang, Junpeng Jiang, Zhan Zhang, Dong Han, Kun Zhan,
                      Xueyang Zhang, XianPeng Lang, <strong>Haiyang Sun</strong>, Xia Zhou, Di Lin, Kaicheng Yu
                      <br>
                      <em> Conference on Artificial Intelligence (AAAI)</em>, 2026
                      <br>
                      <a href="https://arxiv.org/abs/2511.13297">Paper</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/bat.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>BAT: Learning Event-based Optical Flow with Bidirectional Adaptive Temporal
                        Correlation</papertitle>
                      <br>
                      Gangwei Xu, Haotong Lin, Zhaoxing Zhang, Hongcheng Luo, <strong>Haiyang Sun</strong>, Xin Yang
                      <br>
                      <em> Conference on Artificial Intelligence (AAAI)</em>, 2026
                      <br>
                      <a href="https://arxiv.org/abs/2503.03256">Paper</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/PPD.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers</papertitle>
                      <br>
                      Gangwei Xu, Haotong Lin, Hongcheng Luo, Xianqi Wang, Jingfeng Yao, Lianghui Zhu, Yuechuan Pu,
                      Cheng Chi, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Hangjun Ye, Sida Peng, Xin Yang‚Ä†
                      <br>
                      <em> Conference on Neural Information Processing Systems (NeurIPS)</em>, 2025
                      <br>
                      <a href="https://arxiv.org/pdf/2510.07316">Paper</a> /
                      <a href="https://github.com/gangweix/pixel-perfect-depth">Code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/genesis.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal
                        Consistency</papertitle>
                      <br>
                      Xiangyu Guo, Zhanqian Wu, Kaixin Xiong, Ziyang Xu, Lijun Zhou, Gangwei Xu, Shaoqing Xu,
                      <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Hangjun Ye, Wenyu Liu, Xinggang Wang
                      <br>
                      <em> Conference on Neural Information Processing Systems (NeurIPS)</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2506.07497">Paper</a> /
                      <a href="https://github.com/xiaomi-research/genesis">Code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/3drealcar.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>3drealcar: An in-the-wild rgb-d car dataset with 360-degree views</papertitle>
                      <br>
                      Xiaobiao Du, Yida Wang, <strong>Haiyang Sun</strong>, Zhuojie Wu, Hongwei Sheng, Shuyun Wang,
                      Jiaying Ying, Ming Lu, Tianqing Zhu, Kun Zhan, Xin Yu
                      <br>
                      <em> International Conference on Computer Vision (ICCV)</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2406.04875">Paper</a> /
                      <a href="https://github.com/xiaobiaodu/3DRealCar_Toolkit">Code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/posepolit.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth
                      </papertitle>
                      <br>
                      Bu Jin, Weize Li, Baihan Yang, Zhenxin Zhu, Junpeng Jiang, Huan-ang Gao, <strong>Haiyang
                        Sun</strong>, Kun Zhan, Hengtong Hu, Xueyang Zhang, Peng Jia, Hao Zhao
                      <br>
                      <em> International Conference on Intelligent Robots and Systems (IROS)</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2505.01729">Paper</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/s2track.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object Tracking
                      </papertitle>
                      <br>
                      Tao Tang, Lijun Zhou, Pengkun Hao, Zihang He, Kalok Ho, Shuo Gu, Zhihui Hao, <strong>Haiyang
                        Sun</strong>, Kun Zhan, Peng Jia, XianPeng Lang, Xiaodan Liang
                      <br>
                      <em> International Conference on Machine Learning (ICML)</em>, 2025
                      <br>
                      <a href="https://arxiv.org/pdf/2406.02147">Paper</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/BEV-TSR.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Bev-tsr: Text-scene retrieval in bev space for autonomous driving</papertitle>
                      <br>
                      Tao Tang, Dafeng Wei, Zhengyu Jia, Tian Gao, Changwei Cai, Chengkai Hou, Peng Jia, Kun Zhan,
                      <strong>Haiyang Sun</strong>, Fan JingChen, Yixing Zhao, Xiaodan Liang, Xianpeng Lang, Yang Wang
                      <br>
                      <em> Conference on Artificial Intelligence (AAAI)</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2401.01065">Paper</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/dreamcar.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>DreamCar: Leveraging Car-Specific Prior for In-the-Wild 3D Car Reconstruction
                      </papertitle>
                      <br>
                      Xiaobiao Du, <strong>Haiyang Sun</strong>, Ming Lu, Tianqing Zhu, Xin Yu
                      <br>
                      <em> IEEE Robotics and Automation Letters (RA-L)</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2407.16988">Paper</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/StreetGS.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Street Gaussians: Modeling Dynamic Urban Scenes with Gaussian Splatting</papertitle>
                      <br>
                      Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, <strong>Haiyang Sun</strong>, Kun Zhan,
                      Xianpeng Lang, Xiaowei Zhou, Sida Peng.
                      <br>
                      <em> European Conference on Computer Vision (ECCV)</em>, 2024
                      <br>
                      <a href="https://arxiv.org/abs/2401.01339">Paper</a> /
                      <a href="https://github.com/zju3dv/street_gaussians">code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/TOD3Cap.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes</papertitle>
                      <br>
                      Bu Jin, Yupeng Zheng, Pengfei Li, Weize Li, Yuhang Zheng, Sujie Hu, Xinyu Liu, Jinwei Zhu, Zhijie
                      Yan,<strong>Haiyang Sun</strong>, Kun Zhan, Peng Jia, Xiaoxiao Long, Yilun Chen, Hao Zhao
                      <br>
                      <em> European Conference on Computer Vision (ECCV)</em>, 2024
                      <br>
                      <a href="https://arxiv.org/abs/2403.19589">Paper</a> /
                      <a href="https://github.com/jxbbb/TOD3Cap">code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/Open-sight.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>OpenSight: A simple open-vocabulary framework for LiDAR-based object detection
                      </papertitle>
                      <br>
                      Hu Zhang, Jianhua Xu, Tao Tang, <strong>Haiyang Sun</strong>, Xin Yu, Zi Huang, Kaicheng Yu
                      <em> European Conference on Computer Vision (ECCV)</em>, 2024
                      <br>
                      <a href="https://arxiv.org/abs/2312.08876">Paper</a>
                      <p></p>
                    </td>
                  </tr>
                </tbody>
              </table>



              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <heading>Competition</heading>
                      <p>
                        <!--My research interests are in computer vision&#x1F441; and robotics&#x1f916. Recently I am studying deep learning methods on object pose estimation for point cloud and RGB data. -->
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/realADsim-2.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</papertitle>
                      <br>
                      KaiyuanTan, Yingying Shen, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Hangjun Ye
                      <br>
                      <em>[üèÜ<strong>champion</strong>!] Winner of <a
                          href="https://huggingface.co/spaces/XDimLab/ICCV2025-RealADSim-NVS"> the RealADSim workshop
                        </a> @ ICCV 2025 </em>,
                      <br>
                      <a href="https://www.arxiv.org/abs/2510.18341">Paper</a> /
                      <a href=" https://github.com/wm-research/ViSE">code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/Dive.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Dive: Dit-based video generation with enhanced control</papertitle>
                      <br>
                      Junpeng Jiang, Gangyi Hong, Lijun Zhou, Enhui Ma, Hengtong Hu, Xia Zhou, Jie Xiang, Fan Liu,
                      Kaicheng Yu, <strong>Haiyang Sun</strong>, Kun Zhan, Peng Jia, Miao Zhang
                      <br>
                      <em>[üèÜ<strong>champion</strong>!] Winner of <a
                          href="https://coda-dataset.github.io/w-coda2024/track2/"> the W-CODA workshop </a> @ ECCV 2024
                      </em>,
                      <br>
                      <a href="https://arxiv.org/pdf/2409.01595">Paper</a>
                      <p></p>
                    </td>
                  </tr>

                </tbody>
              </table>


              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <heading>Others</heading>
                      <p>
                        papers under review
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/drivelaw.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>DriveLaW: Unifying Planning and Video Generation in a Latent Driving World
                      </papertitle>
                      <br>
                      Tianze Xia, Yongkang Li, Lijun Zhou, Jingfeng Yao, Kaixin Xiong, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Kun Ma, Guang Chen, Hangjun Ye, Wenyu Liu, Xinggang Wang
                      <br>
                      <em> arXiv</em>, 2025
                      <br>
                      <a href="https://arxiv.org/pdf/2512.23421">Paper</a> /
                      <a href="https://wm-research.github.io/DriveLaW/">code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/mirage.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Mirage: One-Step Video Diffusion for Photorealistic and Coherent Asset Editing in Driving Scenes
                      </papertitle>
                      <br>
                      Shuyun Wang, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Hangjun Ye, Xin Yu
                      <br>
                      <em> arXiv</em>, 2025
                      <br>
                      <a href="https://arxiv.org/pdf/2512.24227">Paper</a> /
                      <a href="https://github.com/wm-research/mirage">code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/parkgaussian.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking
                      </papertitle>
                      <br>
                      Xiaobao Wei, Zhangjie Ye, Yuxiang Gu, Zunjie Zhu, Yunfei Guo, Yingying Shen, Shan Zhao, Ming Lu, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Rongfeng Lu, Hangjun Ye
                      <br>
                      <em> arXiv</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2601.01386">Paper</a> /
                      <a href="https://wm-research.github.io/ParkGaussian/">code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/DGGT.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images
                      </papertitle>
                      <br>
                      Xiaoxue Chen, Ziyi Xiong, Yuantao Chen, Gen Li, Nan Wang, Hongcheng Luo, Long Chen,
                      <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Hangjun Ye, Hongyang Li, Ya-Qin Zhang, Hao
                      Zhao
                      <br>
                      <em> arXiv</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2512.03004">Paper</a> /
                      <a href="https://github.com/xiaomi-research/dggt">code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/extrags.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>ExtraGS: Geometric-Aware Trajectory Extrapolation with Uncertainty-Guided Generative
                        Priors</papertitle>
                      <br>
                      Kaiyuan Tan, Yingying Shen, Haohui Zhu, Zhiwei Zhan, Shan Zhao, Mingfei Tu, Hongcheng Luo,
                      <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Hangjun Ye
                      <br>
                      <em> arXiv</em>, 2025
                      <br>
                      <a href="https://arxiv.org/pdf/2508.15529">Paper</a> /
                      <a href="https://github.com/wm-research/extrags">code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/drivemrp.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk
                        Prediction</papertitle>
                      <br>
                      Zhiyi Hou, Enhui Ma, Fang Li, Zhiyi Lai, Kalok Ho, Zhanqian Wu, Lijun Zhou, Long Chen, Chitian
                      Sun, <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Guang Chen, Hangjun Ye, Kaicheng Yu
                      <br>
                      <em> arXiv</em>, 2025
                      <br>
                      <a href="https://arxiv.org/pdf/2507.02948">Paper</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/uni-gaussians.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Uni-gaussians: Unifying camera and lidar simulation with gaussians for dynamic driving
                        scenarios</papertitle>
                      <br>
                      Zikang Yuan, Yuechuan Pu, Hongcheng Luo, Fengtian Lang, Cheng Chi, Teng Li, Yingying Shen,
                      <strong>Haiyang Sun</strong>‚Ä†, Bing Wang, Xin Yang
                      <br>
                      <em> arXiv</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2503.08317">Paper</a> /
                      <a href="https://xiaomi-research.github.io/uni-gaussians/">code</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/cogen.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Cogen: 3d consistent video generation via adaptive conditioning for autonomous driving
                      </papertitle>
                      <br>
                      Yishen Ji, Ziyue Zhu, Zhenxin Zhu, Kaixin Xiong, Ming Lu, Zhiqi Li, Lijun Zhou, <strong>Haiyang
                        Sun</strong>‚Ä†, Bing Wang, Tong Lu
                      <br>
                      <em> arXiv</em>, 2025
                      <br>
                      <a href="https://arxiv.org/abs/2503.22231">Paper</a>
                      <p></p>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <img src='images/Delphi.png' width="100%">
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Unleashing generalization of end-to-end autonomous driving with controllable long
                        video generation</papertitle>
                      <br>
                      Enhui Ma, Lijun Zhou, Tao Tang, Zhan Zhang, Dong Han, Junpeng Jiang, Kun Zhan, Peng Jia, Xianpeng
                      Lang, <strong>Haiyang Sun</strong>, Di Lin, Kaicheng Yu
                      <br>
                      <em> arXiv</em>, 2024
                      <br>
                      <a href="https://arxiv.org/abs/2406.01349">Paper</a> /
                      <a href="https://github.com/westlake-autolab/Delphi">code</a>
                      <p></p>
                    </td>
                  </tr>
                </tbody>
              </table>

              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                  <tr>
                    <td>
                      <heading>Experience</heading>
                    </td>
                  </tr>
                </tbody>
              </table>
              <table width="100%" align="center" border="0" cellpadding="10">
                <tbody>
                  <tr>
                    <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                        src="images/xiaomiEV-logo.png" , width="90%"></td>
                    <td width="80%" valign="center">
                      <b>Xiaomi EV | Â∞èÁ±≥Ê±ΩËΩ¶</b>, China
                      <br> 2024.09 - now
                      <br>
                      <br> <b>Lead Algorithm Expert</b>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                        src="images/LiAuto-logo.png" , width="90%"></td>
                    <td width="80%" valign="center">
                      <b>LiAuto | ÁêÜÊÉ≥Ê±ΩËΩ¶</b>, China
                      <br> 2022.12 - 2024.09
                      <br>
                      <br> <b>Senior Algorithm Expert</b>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                        src="images/alibaba-damo-logo.webp" , width="90%"></td>
                    <td width="80%" valign="center">
                      <b>Alibaba DAMO Academy | ÈòøÈáåËææÊë©Èô¢</b>, China
                      <br> 2017.12 - 2022.12
                      <br>
                      <br> <b>Algorithm Expert</b>
                    </td>
                  </tr>
                  <tr>
                  <tr>
                    <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                        src="images/enhang-logo.jpg" , width="90%"></td>
                    <td width="80%" valign="center">
                      <b>EHang | ‰∫øËà™Êô∫ËÉΩ</b>, China
                      <br> 2016.07 - 2017.12
                      <br>
                      <br> <b>Senior Algorithm Engineer</b>
                    </td>
                  </tr>
                  <tr>
                  <tr>
                    <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                        src="images/tsinghua_logo.png" , width="90%"></td>
                    <td width="80%" valign="center">
                      <b>Tsinghua University</b>, China
                      <br> 2013.09 - 2016.07
                      <br>
                      <br> <b>M.S. Student in Electronic Engineering</b>
                    </td>
                  </tr>
                  <tr>
                    <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                        src="images/BUPT-logo.png" , width="90%"></td>
                    <td width="80%" valign="center">
                      <b>Beijing University of Posts and Telecommunications</b>, China
                      <br> 2009.09 - 2013.07
                      <br>
                      <br> <b>B.S. in Information and Communication Engineering</b>
                    </td>
                  </tr>


                </tbody>
              </table>

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:0px">
                      <br>
                      <p style="text-align:right;font-size:small;">
                        Template stolen from <a href="https://jonbarron.info/">Jon Barron</a>.
                        <br> Last updated: 03/05/2025
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
        </td>
      </tr>
  </table>
</body>

</html>